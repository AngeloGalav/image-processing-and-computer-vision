# Bag of Words representation

[..]

How to represent visual words?
- use a regular grid of positions at several scales or - run an interest points (keypoints) detector (Harris, DoG, Harris affine, MSER, …) Then either - use the patch around the keypoint as “word” - compute its SIFT descriptor to be used as “word” - compute some color descriptor, e.g. mean and variance of subpatches in R, G and B channels
![[visual_words.png]]
Now that the we have the tokens and the visual words, we need to compute the codebook.  

## 2. Compute the codebook
Clustering of extracted patches or descriptors in a finite set of centroids creates the codebook. Usually simple k-means clustering is used.
- This is happening in the space of SIFT descriptors. 
[sift_space]
To each cluster, it will assign a centroid (according to the k-means algorithm). 
If you do this in the patch space instead of the SIFT space, you obtain something like this:
![[patch_space.png]]

This is very similar to do what even the modern CNNs extract in the first layer. With this BOW approach, we are kinda letting the image speak in a sense (the difference is that there's no color in this patches, while with CNNs it is preserving rgb values.)

## 3. Encode training images and new images
Given an image, the same extraction pipeline used to create the codebook (e.g. regular grid + SIFT description of patches) is applied to it. Each extracted descriptor/patch is matched against the codebook and assigned to the nearest centroid in the codebook. In vanilla BoVW, the histogram entry corresponding to such codeword is incremented.

The BOW representation, for a long time, has been 


## 4. Train a classifier

The codewords are essentially features in the image. 
Of course, a lot of time has been spent in applying normalization to this approach. Essentially, normalization is fundemantals if the words are in different sizes
[..]


## VLAD: vector of locally aggregated descriptors
Successful evolutions of BoW focused on making the assignment to a codeword more informative. If descriptors are 𝐷 dimensional (e.g. 128 when using SIFT), and there are 𝐾 codewords, VLAD creates a 𝐷 × 𝐾 descriptor which stores the differences of input words with respect to the codewords, i.e their relative position with respect to the codeword.
![[vlad.png]]
Instead of using a frequency -> i.e. c_i for 4 times, there's a hard assignment. 
... As many entries as you have codewords ...

In this example, ...
As we can see, we don't have histograms anymore since we can have negative values now 
[..]

#### BOVW was the dominant paradigm until 2012
ImageNet initially was made to be very easy to use with BoW approach -> they made a vocalubary of 1000 codewords running k-means on 1 million SIFT descriptors. 

That is, until DL came to be. 

In BoW, the representation is fixed, and there's learning on the hyperparameters of the SVM. 
That is opposed to what happens in Deep learning. 


## Neural Networks
![[Screenshot_20230518_125211.png]]
It is not so different from the standard representation. 

### Activation function
![[Screenshot_20230518_125435.png]]
Why the ..? Because it is very much easy to propagate the backpropagation. 

#### Gradients of activation functions
Gradients play a central role in optimization. Gradient of the output of activation functions with respect to input is very different between sigmoid and ReLU, easier to train (deep) networks when using ReLU. Yet, ReLu can give rise to “dead” neurons, if for all inputs the output is negative.
![[Screenshot_20230518_125629.png]]

As we can see, since the sigmoid function has a gradient, ... wrong output ... difficulty to move -> it is very much difficult to train. 

ReLU is superior to the sigmoid function in this sense -> if you are on the right, you can quickly change output if there's a wrong output, but this is not true to the left (in this case, you completely kill the output). 
- Some neurons may die, they will never recover from producing 0. 

### Activation functions – Leaky ReLU
To avoid dead neurons, a small response can be produced also for negative inputs. The simplest variant is called Leaky ReLU, which has a small but non-zero constant gradient also for negative inputs.
![[leaky_relu.png]]
However, sometimes changing activation functions is not enough to improve our results. 


#### Why do we insert activation functions?

Without activation functions, _we end-up again with a linear classifier_.

#### Is ReLU enough?
I'm computing new coordinates for my points according to where they stand with the hyperplanes of the W. 
However, if did not have a line that separated the points in the first case, I won't have such things also in the second space. 
- -> A linear or affine mapping _does not change linear separability_.

With ReLU, points are collapsed on the two axis.
In this way, we can easily separate points. 
- It is _highly non-linear_.

### Graphical representation & terminology

### Deep Neural Network
L layers in the network, then the network is consider to be deep, 
Number of layers is the depth of the network If 𝐿 > 2, we consider the network to be “deep”. 

#### Width of neural networks
The number of dimensions computed by each layer, i.e. the dimensionality of $ℎ_𝑖$ , is the _width_ of the network. 

### Why “neural” networks?
Better to think it as matrices than neurons:
![[real_dnn.png]]
$b$ is a threshold in a sense.
What we've just seen corresponds to this:
![[DNN_formula.png]]
Which is basically this in a figurative sense:
![[Linear_percep.png]]


### Depth as an exponential gain
If one hidden layer is enough, why using more than one? For instance, XOR can be seen as the special case with only two inputs of a function called parity. In the worst case, which happens for the parity function, the number of hidden units grows exponentially with the input dimensions (it requires one hidden unit corresponding to each input configuration that needs to be distinguished). If more hidden layers are used, instead, a _linear_ number of neurons wrt the input size can learn parity. Well, maybe…


## Vertical edge detection with FC layer
Let’s assume that, to solve the task, the first FC layer would need to detect some kind of local features (e.g. edges, corners, blobs..). What should the first layer matrix 𝑊1 be if a “good” ℎ1 representation are vertical edges?

I want our NN to do this:
![[difference.png]]

## Convolutions
CNN allow to embed the knowlegde ... by ... information is local. 


## Correlation or convolution?
Nobody does flip the kernel...
Cross-correlation, but we call it convolution. 

