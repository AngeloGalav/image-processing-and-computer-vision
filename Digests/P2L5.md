# Successful Architectures
## AlexNet
[..]



## ZFNET
![[ZFNET_clear.png]]
![[ZFNET.png]]
__ZFNET__ tries to reduce the â€œ_trial and error_â€ approach to network design, by introducing _powerful visualizations_ for layers other than the first one. 
They found that
- Aggressive strides
- Large filters
Are very bad! (_dead filters_ in __1__$st$ layer filters, _aliasing artifcats_ in __2__$nd$ layer activations)

So:
- $ğŸ• Ã— ğŸ•$ convs, _stride 2_ in the 1st layer 
- $ğŸ“ Ã— ğŸ“$ convs, _stride 2_ in the 2nd layer.

Aside from this, it has the same architectures of AlexNet (5 conv layers, 3 fc layers)

----
## VGG
VGG commits to explore the effectiveness of simple design choices, and uses:
- 3x3 convolutions, S=1, P=1 
- 2x2 max-pooling, S=2, P=0 
	- $n_{channels}$ _doubles_ after each pool 

Since BN was not invented yet, it uses pre-initialization extensively.

#### Stages
VGG also introduces the concept of __stages__: a _fixed combination of layers_ that process activations at the _same spatial resolution_, which are then repeated.
In VGG, stages are either: 
- conv-conv-pool 
- conv-conv-conv-pool 
- conv-conv-conv-conv-pool

smaller, but more kernel have the same receptive field as large convolution (less params, more non linearities)
- memory for activations doubles though!

#### VGG-16 summary
__Bigger__ network than _AlexNet_
- _138 M params_ (2.3x AlexNet) again, _mostly in fc layers_.
- _~ğŸ’ Tflops_ (~ 14x), 31 Gflops/img, mainly due to convolutions 
- ~ğŸğŸ”. ğŸ“ GB of memory (~ 12x) 
	- _Memory mostly stores activations_ rather than parameters. 
	 - This is also due to the fact that there are __no stem layers__, inputs are processed in the same resolution as they come to. 

## Inception-v1
Based on being quick and efficient. 
- Does not use multiple dense layers, but only one. 

