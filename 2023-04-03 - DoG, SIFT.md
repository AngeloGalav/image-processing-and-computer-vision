## Difference of Gaussian
The DoG uses a _stack of images_, smoothed at different $\sigma$. This is clearly seen in the image below. 
 
We compute several Gaussian smoothed function within an octave. Then, for each _pair_, we take the _differences_. 
- For 5 level of smoothing, we compute (for example) 4 DoG responses.
- We search for blobs in the stack of DoGs. 
![[dogs_octave.png]]
In this picture, $k = 2$. (since we go from $\sigma$ to $2\sigma$).

Now, to find the extrema, we need [[2023-03-13 - ED pt1, NMS, Canny's, Hysteresis#Non-maxima Suppresion (NMS)|NMS]]. NMS, as we know, is computed in a _neighbourhood_.
How do we find this neighbourhood? Remember, our domain now is not only comprised of the $(x, y)$ space, but also of the _scale space_ $\sigma$ dimension.
A neighbourhood of a layer, in the scale space, is made of the upper and lower layer. 
This measn that in total, I'd have only _2 levels of extrema_, which of course is a bit limiting. 

To solve this, we add some __padding__, some _additional layers_.  
- Now, features can appear in images at _4 scale levels_, since I have _4 levels of extrema_. 
	- Notice how now we are computing the extrema at all levels.

![[dogs_octave_2.png]]
Keep in mind, the number of scale levels is $s$. 

Now, there's a problem. If we increase $\sigma$ too much, the kernel also gets _too big_. ($k = \lceil 3\sigma\rceil$)
Thus, rather than modifying $k$, we _shrink the image_, by removing half the rows and half the columns. 
- The size of the kernel stays the same, and we're still computing large scale features. 


### Keypoint detection
__Extrema detection__: a point $(x,y,σ)$ is detected as a _keypoint_ iff its DoG is _higher_ (lower) than that of the __26__ _neighbours_ (8 at the same scale and __18=9+9__ at the two nearby scales) in the $(x,y,σ)$ space.

According to the original article (parameter tuning):
- the best _number of scales_ within an octave is $s=3$ , which translates into 5 levels of filtering. 
- initial $σ$ for each octave should be $\sigma =1.6$ 
- the input image is enlarged by _a factor of 2_ in both dimensions.

## Exemplar DoG keypoints
- Extrema featuring weak DoG response turn out scarcely repeatable…_prune weak responses_. (i.e. points in the sky)
- Lowe also notices that _unstable keypoints_ featuring a sufficiently strong DoG may be _found along edges_ and devises a further pruning step.
	- As we know, we don't want edges since they can be match with several different points in other images. 
![[dog_keypoints_example.png]]

#### Exemplar DoG keypoints in practice
The _size_ of the circle is proportional to the _scale of the feature_ ($σ$), in particular, it's proportional to the $\sigma$ which is found to be an _extrema_ (the detector provides you with this information).
![[extrema.png]]

##### How to have rotation invariance
We need to identify a _prominent direction_ inherent to the patch (i.e. the canonical orientation) and, based on such _direction_, define a _local reference frame_. 
- A reasonable choice consists in identifying _the direction_ along which _most of the gradient_ is found.

Look at this patches. They are the same patch, just rotated differently. 
To find the direction of our patches, we could use the gradient. 
![[local_pixel_frame.png]]
- When computing the descriptor, pixel coordinates are taken in the _local reference frame_.

In short:
- We compute a local reference frame which define how we should rotate each patch.
- This local reference frame is then rotate into the _canonical orientation_.

## Scale and Rotation Invariance Description
- We need to define a __Scale and Rotation Invariant Description__ 
- Most keypoint detection algorithms follow an approach close to Lowe’s one 
	- i.e. finding extrema across a stack of images computed while increasing a scale parameter 

- Once each _keypoint_ has been extracted, a _surrounding patch_ is considered to compute its _descriptor_ (scale and rotation invariant) 
	- __Scale invariance__: the patch is taken from _the stack of images_ ($L(x, y, \sigma)$ in Lowe’s) that correspond to the characteristic scale. 
	- __Rotation invariance__: a _canonical_ (aka characteristic) _patch orientation_ is computed, so that the descriptor can then be computed on a _canonically-oriented patch_.
		- Orientation wrt a new reference system, not the one of the image.

### Canonical Orientation
How do we define this canonical orientation though?
Lowe proposes to compute the canonical orientation of DoG keypoints as follows: 
Given the _keypoint_, the __magnitude__ and __orientation__ of the _gradient_ are computed at _each pixel_ of the associated _Gaussian-smoothed image_, $L$:
![[canonical_orientation.png]]

To chose which one should be our canonical orientation for a keypoint, we compute an _orientation histogram_ by accumulating the _contributions_ of the pixels belonging to a _neighborhood_ of the _keypoint location_ (bin size equal to 10°)
	- The contribution of each pixel to its designated orientation bin is given by the gradient magnitude weighted by a Gaussian with $σ=1.5 \cdot σ_s$ (σs denotes the scale of the keypoint) 

The __canonical direction__ will be the one along which _most of the gradient_ is found (_sense most of the change_).

The __characteristic__ __orientation__ (meaning, canonical) of the keypoint is given by the _highest peak_ of the _orientation histogram_.
- Other peaks _higher_ than 80% of the main one would be kept as well (i.e., they are also considered as canonical orientations).

A keypoint may have _multiple canonical orientations_ and, in turn, _multiple descriptors_ sharing the same location/scale with diverse orientations.
- This has been found to occur quite rarely, about 15% of the keypoints
![[histogram.png]]
Finally, a _parabola is fit_ in the neighbourhood of _each peak_ to achieve a more accurate estimation of the canonical orientation.
- The two bins (sets of columns) adjacent to the found _peak_ are considered.

## SIFT Descriptor
I'm going to orient according to the canonical orientation. 

The __SIFT (Scale Invariant Feature Transform) descriptor__ is computed as follows: 
- A 16x16 oriented _pixel grid_ around each keypoint is considered 
	- This patch is of the same dimension for every keypoint. 
- This is _further divided_ into 4x4 regions (each of size 4x4 pixels)
	- [I think the professors meant that they are grouped into 4x4 regions].
- A _gradient orientation histogram_ is created for each 4x4 region.
- Each histogram has 8 bins 
	- (i.e. bin size 45°, each bin represents an angle and they progress by 45˚ (i.e, 0,45,90,135...), thus we have _8 directions_ per region in the histogram).
- Each pixel in the region contributes to its designated bin according to 
	- __Gradient magnitude__ 
	- __Gaussian weighting__ function _centered_ at the _keypoint_ (with $σ$ equal to _half the grid size_)

![[sift.png]]
The __descriptor size__ is given by the _number of regions_ times the _number of histogram bins_ _per region_, i.e. 4x4x8 = 128 (histograms are concatenated).

In short: a SIFT descriptor is a set of histograms. 

SIFT is biologically inspired: there exist studies suggesting that neurons in the primary visual cortex (V1) do match gradient orientations robustly with respect to a certain degree of shift of the input pattern for recognition purposes.

He also decided to weight _how the orientation_ contributes to different bins. 
So, in the picture, $b_k$ and $b_{k+1}$ are 2 bins. $d_k$ and $d_{k+1}$ are the distance from the center of the 2 bins. 

To avoid boundary effects, a _soft_ rather than hard assignment is employed in SIFT, whereby the _contribution_ to two adjacent bins is weighted by the _distance_ to the _bin center_:
![[sift2.png]]
- He also decided to weight _how the orientation_ contributes to different bins. 
- So, in the picture, $b_k$ and $b_{k+1}$ are 2 bins. $d_k$ and $d_{k+1}$ are the distance from the center of the 2 bins. The result weights are computed as in the picture.

This is done within _an histogram_ as well as _between regions_ (the contribution is spread bilinearly between 4 adjacent regions). Hence, the overall scheme is referred to as _trilinear interpolation_. 
- The _descriptor is normalized_ to unit length to gain __invariance__ wrt __affine intensity__ changes.
- To _increase_ robustness to non-linear changes, all elements larger than 0.2 are _saturated_ and the _descriptor normalized again_.
	- you get a bit of robustness to non-linear complexity change. 

## Matching Process
_Descriptors_ (e.g. SIFT) are _compared_ across diverse _views of a scene_ to find corresponding keypoints.
This is a classical __Nearest Neighbour (NN) Search__ problem: 
- Given a set $S$ of points $p_i$ , in a metric space $M$ and a query point $q ∈ M$, find the $p_i$ closest to $q$. 

We wish to match the _local features_ computed from an _image under analysis_ (_target image_, $T$) to those already computed from a _reference image_ ($R$) or a set of reference images.
![[matching_process.png]]
For each feature in $T$ we look for the most similar one in $R$: 
- The features in $T$ represent the _query points_, $q$ 
- The features in $R$ provide _set_ $S$ of points $p_i$.  
- When matching SIFT descriptors the _distance_ typically used is the __Euclidean distance__.

## Validating Matches
The found NN does not necessarily provide a valid correspondence as some features in $T$ may not have a corresponding feature in $R$ (clutter and/or viewpoint changes).
- Enforce criteria to accept/reject a match found by the NN search process. Simplest thing to do is to use a _threshold_
![[thres_NN.png]]

For the 2nd criteria to work (that is always less than 1),  the _ratio_ should be _small_ 
- the numerator must be small and the denominator must be large 

- Lowe shows that $T=0.8$ may allow for rejecting 90% of wrong matches while missing only 5% of those correct

## Efficient NN-Search
Exhaustively searching for the Nearest Neighbour of the query feature, $q$, has linear complexity in the size of $S$.
__This is slow though!__

Efficient _indexing_ techniques are exploited to _speed-up_ the NN-search process.
- The main indexing technique exploited for feature matching is known as __k-d tree__, in particular, the preferred approach is the _approximate variant_ referred to as _Best Bin First_ (BBF). 
- Unlike the basic k-d tree, the _BBF_ is efficient also in _high-dimensional spaces_ such as _descriptor spaces_.


## A few other major proposals
- A fast alternative to SIFT is SURF (Speeded-Up Robust Features) algorithm. Blob-like features are detected through efficiently computable filters inspired by Lindberg’s Gaussian derivatives. The scale-space is achieved more efficiently by mean filtering

- MSER (Maximally Stable Extremal Regions) detects interest regions of arbitrary shapes, in particular approximately uniform areas either brighter or darker than their surroundings. Description of MSER region may then be carried out using SIFT 
 
- FAST (Features from Accelerated Segment Test) is a very efficient detector of corner-like features.

- A variety of binary descriptors, such as BRIEF, ORB and BRISK, have been proposed to minimize memory occupancy and accelerate the matching step thanks to the use of the Hamming distance