#### Revision of the last lesson: some interesting stuff 
We have an idea of equivariance, meaning that a picture of a cat remains a picture of a cat, no matter how it is oriented etc...

The convolutions shouldnt be ..., we may only use it in items that respect some constraints. If for some reason they dont, we may have to change strategy.

## Convolution as matrix multiplication
[.. add part from slides in between]

## Activation
We can add more than one kernel ...

## Convolutional layer
![[convolutional_layer.png]]
If we have 4 filters, each of size 3 Ã— 5 Ã— 5, we can describe the overall operation realized by the layer as

In the general case, we compute $ð‘ª_{ð’ð’–ð’•}$ convolutions between vector-valued kernels and input activations.
- The number of output channels is equal to the number of kernel. 

## Relationship between spatial dimensions
In general:
$ð»_{ð‘œð‘¢ð‘¡} = ð»_{ð‘–ð‘›} âˆ’ ð»_{ð‘˜} + 1$
$ð‘Š_{ð‘œð‘¢ð‘¡} = ð‘Š_{ð‘–ð‘›} âˆ’ ð‘Š_{ð‘˜} + 1$
If we stack several convolution layers, feature maps will shrink after each layer. The absence of padding is also referred to as `padding=â€œvalidâ€` (meaning there's no padding).

## Zero Padding
Common â€œsolutionâ€ is to add zero padding around the original image.
In this way, we preserve the dimension of the input into the final image. 

#### Why using 0?
Usually, using 0 is as good as using anything else. 
For dense task, however, we can use better padding (i.e. repeating columns) which gives may give us better results. Usualy 0 is still good in general. 

[..]
For the very last layer, nothing changes (thats why we have $l-1$)
![[cnn1.png]]
We need strides so that the receptive field can be big enough, without having to 


If the GPU supports MAC, multiplication and addition can be done in one cycle, thus dropping the 2. 
It is important to understand if a paper is talking about FLOPS or MACS. 

## Other common convolutions

- 1D convolutions
- 3D convolutions are usually used in videos.

## Pooling layers
Avg-pooling makes no sense to use since we can sue 
Max-pooling is the only pooling that makes sense to use, since it would require mre than a kernel.  

Since pooling is an hyperparameter, it can downsample, but it doesn't have to. 

Max-pooling is an apriori decision: since it is not learned (and thus it is more effiencient to use) i dont really know if our model _should_ really use this operation. 

This is how Max-pooling would be implemented if we had to use kernels:
![[max_pooling.png]]

Downsampling comes from stride, not from pooling: we can achieve it with convolutions as well. Yet, compared to strided convolutions, max pooling o has no learnable parameters (pro and con) o provides invariance to small spatial shifts.

### A tensor is flattened at the end
![[cnn_flatten.png]]
I flatten because at a certain point a need a global vision of the image (a global receptive field), and the only way to do that is by using a _linear, fully connected layer_.

>[!WARNING]
>__Remeber!!__: we can 
16 kernels, 5x5x6 